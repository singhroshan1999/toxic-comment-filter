{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('train_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SIZE = 20000\n",
    "def stemmer(data):\n",
    "    stem = PorterStemmer()\n",
    "    corpus = []\n",
    "    count = 0\n",
    "    for i in data:\n",
    "        if count%(data.size/50) == 0:\n",
    "            print(count)\n",
    "        count+=1\n",
    "        post = re.sub('[^a-zA-Z]',' ',i)\n",
    "        post = post.lower()\n",
    "        post =  post.split()\n",
    "        post = [stem.stem(word) for word in post if not word in set(stopwords.words('english'))]\n",
    "        post = ' '.join(post)\n",
    "        corpus.append(post)\n",
    "    return corpus\n",
    "def stemmer_s(strin):\n",
    "    stem = PorterStemmer()\n",
    "    corpus = []\n",
    "    post = re.sub('[^a-zA-Z]',' ',strin)\n",
    "    post = post.lower()\n",
    "    post =  post.split()\n",
    "    post = [stem.stem(word) for word in post if not word in set(stopwords.words('english'))]\n",
    "    post = ' '.join(post)\n",
    "    corpus.append(post)\n",
    "    return corpus\n",
    "def save(mdlName,classifier,count_vector):\n",
    "    clsf = open(mdlName+'.mdl','wb')\n",
    "    cnt = open(mdlName+'_count_vector.mdl','wb')\n",
    "    pickle.dump(classifier,clsf)\n",
    "    pickle.dump(count_vector,cnt)\n",
    "def train(dataset,do_save = False):\n",
    "    corpus = stemmer(dataset['comment_text'][:DATA_SIZE])\n",
    "    count_vector = CountVectorizer(max_features = 40000)\n",
    "    x = count_vector.fit_transform(corpus).toarray()\n",
    "    y = []\n",
    "    for i in dataset.iloc[:DATA_SIZE,-1].values:\n",
    "        if i > 0:\n",
    "            y.append(1)\n",
    "        else:\n",
    "            y.append(0)\n",
    "    classifier = RandomForestClassifier(n_estimators= 100,criterion = 'gini',random_state = 0)\n",
    "    classifier.fit(x,y)\n",
    "    if do_save:\n",
    "        import random\n",
    "        filename = 'toxic'+str(random.randint(1000,9999))\n",
    "        save(filename,classifier,count_vector)\n",
    "    return classifier,count_vector\n",
    "def load(mdlName):\n",
    "    clsf = open(mdlName+'.mdl','rb')\n",
    "    cnt = open(mdlName+'_count_vector.mdl','rb')\n",
    "    return pickle.load(clsf),pickle.load(cnt)\n",
    "def isToxic(s,classifier,count_vector):\n",
    "     return (classifier.predict(count_vector.transform(stemmer_s(s)))[0] != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(dataset):\n",
    "    data_low = dataset\n",
    "    corpus_low = stemmer(data_low['comment_text'][:DATA_SIZE])\n",
    "    count_vector = CountVectorizer(max_features = 40000)\n",
    "    x = count_vector.fit_transform(corpus_low).toarray()\n",
    "    y = []\n",
    "    for i in data_low.iloc[:DATA_SIZE,-1].values:\n",
    "        if i > 0:\n",
    "            y.append(1)\n",
    "        else:\n",
    "            y.append(0)\n",
    "    x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.20,random_state=0)\n",
    "    classifier = RandomForestClassifier(n_estimators= 100,criterion = 'gini',random_state = 0)\n",
    "    classifier.fit(x_train,y_train)\n",
    "    y_pred = classifier.predict(x_test)\n",
    "    cm = confusion_matrix(y_test,y_pred)\n",
    "    print(cm)\n",
    "    c = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] != y_test[i]:\n",
    "            print(y_pred[i],y_test[i],x_test[i])\n",
    "            c+=1\n",
    "            print(c)\n",
    "    return classifier,count_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl,co = show(dataset)\n",
    "isToxic('bitch',classifier=cl,count_vector=co)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "400\n",
      "800\n",
      "1200\n",
      "1600\n",
      "2000\n",
      "2400\n",
      "2800\n",
      "3200\n",
      "3600\n",
      "4000\n",
      "4400\n",
      "4800\n",
      "5200\n",
      "5600\n",
      "6000\n",
      "6400\n",
      "6800\n",
      "7200\n",
      "7600\n",
      "8000\n",
      "8400\n",
      "8800\n",
      "9200\n",
      "9600\n",
      "10000\n",
      "10400\n",
      "10800\n",
      "11200\n",
      "11600\n",
      "12000\n",
      "12400\n",
      "12800\n",
      "13200\n",
      "13600\n",
      "14000\n",
      "14400\n",
      "14800\n",
      "15200\n",
      "15600\n",
      "16000\n",
      "16400\n",
      "16800\n",
      "17200\n",
      "17600\n",
      "18000\n",
      "18400\n",
      "18800\n",
      "19200\n",
      "19600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
       "                        warm_start=False),\n",
       " CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=40000, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, vocabulary=None))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(dataset,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier2,count_vector2 = load('toxic7825')\n",
    "# isToxic('bitch',classifier2,count_vector2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
