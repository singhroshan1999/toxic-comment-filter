{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('train_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SIZE = 100\n",
    "def stemmer(data):\n",
    "    stem = PorterStemmer()\n",
    "    corpus = []\n",
    "    count = 0\n",
    "    for i in data:\n",
    "        if count%(data.size/50) == 0:\n",
    "            print(count)\n",
    "        count+=1\n",
    "        post = re.sub('[^a-zA-Z]',' ',i)\n",
    "        post = post.lower()\n",
    "        post =  post.split()\n",
    "        post = [stem.stem(word) for word in post if not word in set(stopwords.words('english'))]\n",
    "        post = ' '.join(post)\n",
    "        corpus.append(post)\n",
    "    return corpus\n",
    "def stemmer_s(strin):\n",
    "    stem = PorterStemmer()\n",
    "    corpus = []\n",
    "    post = re.sub('[^a-zA-Z]',' ',strin)\n",
    "    post = post.lower()\n",
    "    post =  post.split()\n",
    "    post = [stem.stem(word) for word in post if not word in set(stopwords.words('english'))]\n",
    "    post = ' '.join(post)\n",
    "    corpus.append(post)\n",
    "    return corpus\n",
    "def save(mdlName,classifier,count_vector):\n",
    "    clsf = open(mdlName+'.mdl','wb')\n",
    "    cnt = open(mdlName+'_count_vector.mdl','wb')\n",
    "    pickle.dump(classifier,clsf)\n",
    "    pickle.dump(count_vector,cnt)\n",
    "def train(dataset,do_save = False):\n",
    "    corpus = stemmer(dataset['comment_text'][:DATA_SIZE])\n",
    "    count_vector = CountVectorizer(max_features = 40000)\n",
    "    x = count_vector.fit_transform(corpus).toarray()\n",
    "    y = []\n",
    "    for i in dataset.iloc[:DATA_SIZE,-1].values:\n",
    "        if i > 0:\n",
    "            y.append(1)\n",
    "        else:\n",
    "            y.append(0)\n",
    "    classifier = RandomForestClassifier(n_estimators= 100,criterion = 'gini',random_state = 0)\n",
    "    classifier.fit(x,y)\n",
    "    if do_save:\n",
    "        import random\n",
    "        filename = 'toxic'+str(random.randint(1000,9999))\n",
    "        save(filename,classifier,count_vector)\n",
    "    return classifier,count_vector\n",
    "def load(mdlName):\n",
    "    clsf = open(mdlName+'.mdl','rb')\n",
    "    cnt = open(mdlName+'_count_vector.mdl','rb')\n",
    "    return pickle.load(clsf),pickle.load(cnt)\n",
    "def isToxic(s,classifier,count_vector):\n",
    "     return (classifier.predict(count_vector.transform(stemmer_s(s)))[0] != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(dataset):\n",
    "    data_low = dataset\n",
    "    corpus_low = stemmer(data_low['comment_text'][:DATA_SIZE])\n",
    "    count_vector = CountVectorizer(max_features = 40000)\n",
    "    x = count_vector.fit_transform(corpus_low).toarray()\n",
    "    y = []\n",
    "    for i in data_low.iloc[:DATA_SIZE,-1].values:\n",
    "        if i > 0:\n",
    "            y.append(1)\n",
    "        else:\n",
    "            y.append(0)\n",
    "    x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.20,random_state=0)\n",
    "    classifier = RandomForestClassifier(n_estimators= 100,criterion = 'gini',random_state = 0)\n",
    "    classifier.fit(x_train,y_train)\n",
    "    y_pred = classifier.predict(x_test)\n",
    "    cm = confusion_matrix(y_test,y_pred)\n",
    "    print(cm)\n",
    "    c = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] != y_test[i]:\n",
    "            print(y_pred[i],y_test[i],x_test[i])\n",
    "            c+=1\n",
    "            print(c)\n",
    "    return classifier,count_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl,co = show(dataset)\n",
    "isToxic('bitch',classifier=cl,count_vector=co)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(dataset,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier2,count_vector2 = load('toxic7825')\n",
    "# isToxic('bitch',classifier2,count_vector2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
